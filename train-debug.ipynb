{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search of runtime errors\n",
    "This notebook has exactly the same code as train.py except for the argument parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no lighting pack\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from easydict import EasyDict as edict\n",
    "from lib.cfgs import c as dcfgs\n",
    "import lib.cfgs as cfgs\n",
    "import os\n",
    "os.environ['JOBLIB_TEMP_FOLDER']=dcfgs.shm\n",
    "import argparse\n",
    "os.environ['GLOG_minloglevel'] = '3'\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import sys\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import embed\n",
    "\n",
    "from lib.decompose import *\n",
    "from lib.net import Net, load_layer, caffe_test\n",
    "from lib.utils import *\n",
    "from lib.worker import Worker\n",
    "import google.protobuf.text_format # added to fix missing protobuf properties -by Mario\n",
    "sys.path.insert(0, osp.dirname('__file__')+'/lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step0(pt, model):\n",
    "    net = Net(pt, model=model, noTF=1)  # lib/net.Net instantiate the NetBuilder -by Mario\n",
    "    WPQ, pt, model = net.preprocess_resnet() # WPQ stores pruned values, which will be later saved to the caffemodel -by Mario\n",
    "    return {\"WPQ\": WPQ, \"pt\": pt, \"model\": model}\n",
    "\n",
    "def step1(pt, model, WPQ, check_exist=False):\n",
    "    print(pt)\n",
    "    net = Net(pt, model, noTF=1)\n",
    "    model = net.finalmodel(WPQ) # loads weights into the caffemodel - by Mario\n",
    "    if 1:#TODO: Consider adding a configuration paramter to cfgs.py in order to control whether or not to prune the last conv layer -by Mario\n",
    "        convs = net.convs\n",
    "        redprint(\"including last conv layer!\")\n",
    "    else:\n",
    "        convs = net.convs[:-1]\n",
    "        redprint(\"ignoring last conv layer!\")\n",
    "    if dcfgs.dic.option == 1:\n",
    "        sums = net.type2names('Eltwise')[:-1]\n",
    "        newsums = []\n",
    "        for i in sums:\n",
    "            if not i.endswith('block8_sum'):\n",
    "                newsums.append(i)\n",
    "        newconvs = []\n",
    "        for i in convs:\n",
    "            if i.endswith('_proj'):\n",
    "                newconvs.insert(0,i)\n",
    "            else:\n",
    "                newconvs.append(i)\n",
    "        convs = newsums + newconvs\n",
    "    else:\n",
    "        convs += net.type2names('Eltwise')[:-1]\n",
    "    if dcfgs.dic.fitfc:\n",
    "        convs += net.type2names('InnerProduct')\n",
    "    if dcfgs.model in [cfgs.Models.xception,cfgs.Models.resnet]:\n",
    "        for i in net.bns:\n",
    "            if 'branch1' in i:\n",
    "                convs += [i]\n",
    "    net.freeze_images(check_exist=check_exist, convs=convs)\n",
    "    return {\"model\":model}\n",
    "\n",
    "def combine():\n",
    "    net = Net(dcfgs.prototxt, dcfgs.weights)\n",
    "    net.combineHP()\n",
    "\n",
    "def splitrelu():\n",
    "    net = Net(dcfgs.prototxt, model=dcfgs.weights)\n",
    "    print(net.seperateConvReLU())\n",
    "\n",
    "def addbn(pt='../resnet-cifar10-caffe/resnet-56/prb_mem_bn_trainval.prototxt', model=\"../resnet-cifar10-caffe/resnet-56/snapshot/prb_VH_bn__iter_64000.caffemodel\"):\n",
    "    worker=Worker()\n",
    "    def ad(pt, model):\n",
    "        net = Net(pt, model=model, noTF=1)\n",
    "        #net.computation()\n",
    "        pt, WPQ = net.add_bn()\n",
    "        return {'new_pt': pt, 'model':model, 'WPQ':WPQ}\n",
    "    outs = worker.do(ad, pt=pt, model=model)\n",
    "    worker.do(stepend, **outs)\n",
    "    #stepend(**outs)\n",
    "\n",
    "def compute(pt='../resnet-cifar10-caffe/resnet-56/trainval.prototxt', model=\"../resnet-cifar10-caffe/resnet-56/snapshot/_iter_64000.caffemodel\"):\n",
    "    net = Net(pt, model=model, noTF=1)\n",
    "    net.computation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nBatches  500\n",
      "nBatches_fc  500\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "cfgs.set_nBatches(dcfgs.nBatches)\n",
    "dcfgs.dic.option=1\n",
    "print(\"nBatches \", dcfgs.nBatches)\n",
    "print(\"nBatches_fc \", dcfgs.nBatches_fc)\n",
    "\n",
    "DEBUG_Mario = 1\n",
    "\n",
    "\n",
    "# c3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function call: c3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def c3(pt=cfgs.vgg.model,model=cfgs.vgg.weights):\n",
    "TODO: Consider changing cfgs.vgg.model and cfgs.vgg.weights \n",
    "      (paths to the .prototxt and .caffemodel files) \n",
    "      for a generic model reference -by Mario\n",
    "\"\"\"\n",
    "pt=cfgs.vgg.model\n",
    "model=cfgs.vgg.weights\n",
    "\n",
    "dcfgs.splitconvrelu=True\n",
    "cfgs.accname='accuracy@5' # name of layer in the prototxt -by Mario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "stage0 freeze\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'end of step0 execution'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#worker = Worker()\n",
    "#outputs = worker.do(step0, pt=pt, model=model)\n",
    "\"\"\"\n",
    "step0 execution\n",
    "def step0(pt, model)\n",
    "\"\"\"\n",
    "pt=cfgs.vgg.model\n",
    "model=cfgs.vgg.weights\n",
    "\n",
    "net = Net(pt, model=model, noTF=1)  # lib/net.Net instantiate the NetBuilder -by Mario\n",
    "WPQ, pt, model = net.preprocess_resnet() # WPQ stores pruned values, which will be later saved to the caffemodel -by Mario\n",
    "outputs = {\"WPQ\": WPQ, \"pt\": pt, \"model\": model}\n",
    " \n",
    "printstage(\"freeze\")\n",
    "pt = outputs['pt'] \n",
    "\n",
    "\"\"\"end of step0 execution\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WPQ': {}, 'model': 'temp/bn_vgg.caffemodel', 'pt': 'temp/bn_vgg.prototxt'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/bn_vgg.prototxt\n",
      "\u001b[31mincluding last conv layer!\u001b[0m\n",
      "['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2', 'conv5_3']\n"
     ]
    }
   ],
   "source": [
    "#outputs = worker.do(step1,**outputs)\n",
    "\"\"\"\n",
    "step1 execution\n",
    "def step1(pt, model, WPQ, check_exist=False)\n",
    "\"\"\"\n",
    "check_exist = False\n",
    "print(pt)\n",
    "net = Net(pt, model, noTF=1) # re-instantiate the net object with new pt and model \n",
    "model = net.finalmodel(WPQ) # loads weights into the caffemodel - by Mario\n",
    "\n",
    "if 1:#TODO: Consider adding a configuration paramter to cfgs.py in order to control whether or not to prune the last conv layer -by Mario\n",
    "    convs = net.convs\n",
    "    redprint(\"including last conv layer!\")\n",
    "else:\n",
    "    convs = net.convs[:-1]\n",
    "    redprint(\"ignoring last conv layer!\")\n",
    "\n",
    "print(convs)\n",
    "\n",
    "if dcfgs.dic.option == 1:\n",
    "    if DEBUG_Mario: print(\"this line executed because dcfgs.dic.option is 1\")\n",
    "    sums = net.type2names('Eltwise')[:-1]\n",
    "    newsums = []\n",
    "    for i in sums:\n",
    "        if not i.endswith('block8_sum'):\n",
    "            newsums.append(i)\n",
    "    newconvs = []\n",
    "    for i in convs:\n",
    "        if i.endswith('_proj'):\n",
    "            newconvs.insert(0,i)\n",
    "        else:\n",
    "            newconvs.append(i)\n",
    "    convs = newsums + newconvs\n",
    "else:\n",
    "    convs += net.type2names('Eltwise')[:-1]\n",
    "    \n",
    "if dcfgs.dic.fitfc:\n",
    "    convs += net.type2names('InnerProduct')\n",
    "if dcfgs.model in [cfgs.Models.xception,cfgs.Models.resnet]:\n",
    "    for i in net.bns:\n",
    "        if 'branch1' in i:\n",
    "            convs += [i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---          \n",
    "## Function call:  net.freeze_images(check_exist=check_exist, convs=convs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/frozen500.pickle\n"
     ]
    }
   ],
   "source": [
    "\"\"\"start of net.freeze_images\"\"\"\n",
    "# def freeze_images(self, check_exist=False, convs=None, **kwargs):\n",
    "if cfgs.layer:\n",
    "    frozen = net._frozen_layer\n",
    "    if check_exist: # this if-condition should be implemented\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "else:\n",
    "    frozen = net._frozen\n",
    "    if check_exist:\n",
    "            if osp.exist(frozen):\n",
    "                print(\"Exist\", frozen)\n",
    "print(\"froze: \", frozen)\n",
    "\n",
    "if convs is None:\n",
    "    convs = self.type2names()\n",
    "if cfgs.layer:\n",
    "    #feats_dict, points_dict = net.extract_layers(convs)\n",
    "    print(\"extract_layers (single-layer pruning)\")\n",
    "else:\n",
    "    print(\"extract_features (full pruning)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### Function call: feats_dict, points_dict = net.extract_features(names=convs, save=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run for 500 batches nFeatsPerBatch 100\n",
      "points_dict:  {'nPointsPerLayer': 10, 'nBatches': 500}\n",
      "feats_dict: {}\n"
     ]
    }
   ],
   "source": [
    "#def extract_features(self, names=[], nBatches=None, points_dict=None, save=False):\n",
    "\"\"\"start of net.extract_features\"\"\"\n",
    "nBatches = None\n",
    "names = convs\n",
    "points_dict=None\n",
    "save = 1\n",
    "\n",
    "nBatches = dcfgs.nBatches\n",
    "nPointsPerLayer = dcfgs.nPointsPerLayer\n",
    "\n",
    "if not isinstance(names, list):\n",
    "    names = [names]\n",
    "inner = False\n",
    "\n",
    "if len(names)==1:\n",
    "    print(\"this code only executes for FC layers\")\n",
    "    for top in self.innerproduct:\n",
    "        if names[0] in self.bottom_names[top]:\n",
    "            inner = True\n",
    "            nBatches = dcfgs.nBatches_fc\n",
    "            break\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "pads = dict()\n",
    "shapes = dict()\n",
    "feats_dict = dict()\n",
    "\n",
    "def set_points_dict(name, data):\n",
    "    assert name not in points_dict\n",
    "    points_dict[name] = data\n",
    "\n",
    "dcfgs.data = cfgs.Data.lmdb  # I think this disables the use of Data.pro type of data -by Mario\n",
    "if save:\n",
    "    if points_dict is None:\n",
    "        frozen_points = False\n",
    "        points_dict = dict()\n",
    "        #if 0 and self._mem:\n",
    "        #   self.usexyz()\n",
    "        set_points_dict(\"nPointsPerLayer\", nPointsPerLayer)\n",
    "        set_points_dict(\"nBatches\", nBatches)\n",
    "    else:\n",
    "        frozen_points = True\n",
    "        if nPointsPerLayer != points_dict[\"nPointsPerLayer\"] or nBatches != points_dict[\"nBatches\"]:\n",
    "            print(\"overwriting nPointsPerLayer, nBatches with frozen_points\")\n",
    "\n",
    "        nPointsPerLayer = points_dict[\"nPointsPerLayer\"]\n",
    "        nBatches = points_dict[\"nBatches\"]\n",
    "        \n",
    " \n",
    "\n",
    "assert len(names) > 0 \n",
    "\n",
    "nPicsPerBatch = net.blobs_num(names[0])\n",
    "nFeatsPerBatch = nPointsPerLayer  * nPicsPerBatch\n",
    "\n",
    "print(\"run for\", dcfgs.nBatches, \"batches\", \"nFeatsPerBatch\", nFeatsPerBatch)\n",
    "nFeats = nFeatsPerBatch * nBatches\n",
    "\n",
    "print(\"points_dict: \", points_dict)\n",
    "print(\"feats_dict:\", feats_dict  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points_dict:  {'nPointsPerLayer': 10, 'nBatches': 500}\n",
      "feats_dict: {}\n",
      "nBatches 500\n",
      "nBatches_fc 500\n",
      "nPicsPerBatch 10\n",
      "nPointsPerLayer  10\n",
      "nFeatsPerBatch 100\n",
      "nFeats 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"points_dict: \", points_dict)\n",
    "print(\"feats_dict:\", feats_dict  )\n",
    "print(\"nBatches\", dcfgs.nBatches) #conig defautls\n",
    "print(\"nBatches_fc\", dcfgs.nBatches_fc) # config defaults\n",
    "print(\"nPicsPerBatch\", nPicsPerBatch) # defined in the prototxt and stured in net.num\n",
    "print(\"nPointsPerLayer \", nPointsPerLayer) # config defaults (this shold be call nPointsPerChannelofaLayer)\n",
    "print(\"nFeatsPerBatch\",nFeatsPerBatch )# =  nPointsPerLayer  * nPicsPerBatch\n",
    "print(\"nFeats\", nFeats) # = nFeatsPerBatch * nBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting conv1_1 (50000, 64)\n",
      "Extracting conv1_2 (50000, 64)\n",
      "Extracting conv2_1 (50000, 128)\n",
      "Extracting conv2_2 (50000, 128)\n",
      "Extracting conv3_1 (50000, 256)\n",
      "Extracting conv3_2 (50000, 256)\n",
      "Extracting conv3_3 (50000, 256)\n",
      "Extracting conv4_1 (50000, 512)\n",
      "Extracting conv4_2 (50000, 512)\n",
      "Extracting conv4_3 (50000, 512)\n",
      "Extracting conv5_1 (50000, 512)\n",
      "Extracting conv5_2 (50000, 512)\n",
      "Extracting conv5_3 (50000, 512)\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "\n",
    "    \"\"\"avoiding X out of bound\"\"\"\n",
    "    shapes[name] = (net.blobs_height(name), net.blobs_width(name))\n",
    "\n",
    "    if inner or len(net.blobs_shape(name))==2 or ( shapes[name][0] == 1 and shapes[name][1] == 1):\n",
    "        print(\"this condition should not execute or standard run on VGG\")\n",
    "        if 0: print(name)\n",
    "        chs = net.blobs_channels(name)\n",
    "        if len(net.blobs_shape(name)) == 4:\n",
    "            chs*=shapes[name][0]*shapes[name][1]\n",
    "        feats_dict[name] = np.ndarray(shape=(nPicsPerBatch * dcfgs.nBatches_fc,chs ))\n",
    "    else:\n",
    "        feats_dict[name] = np.ndarray(shape=(nFeats, net.blobs_channels(name)))\n",
    "    print(\"Extracting\", name, feats_dict[name].shape) # for a standard run,  names is a list with the conv layers: name = convs -by Mario\n",
    "\n",
    "idx = 0\n",
    "fc_idx = 0\n",
    "if save:\n",
    "    if not frozen_points: \n",
    "        \"\"\"there is no frozen points, we have to save the one we will prepare\"\"\"\n",
    "        set_points_dict(\"data\", net.data().shape)\n",
    "        set_points_dict(\"label\", net.label().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points_dic was updated:  {'nPointsPerLayer': 10, 'nBatches': 500, 'data': (10, 3, 224, 224), 'label': (10,)}\n"
     ]
    }
   ],
   "source": [
    "print(\"points_dic was updated: \", points_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width and height of conv1_1: (224,224) | net.shapes[layer]: \n",
      "width and height of conv1_1: (224,224) | net.blobs_x(layer): \n"
     ]
    }
   ],
   "source": [
    "# for suding layes' structure\n",
    "this_layer = 'conv1_1'\n",
    "print(\"width and height of %s: (%d,%d) | net.shapes[layer]: \" % (this_layer, shapes['conv1_1'][0], shapes['conv1_1'][1]))\n",
    "print(\"width and height of %s: (%d,%d) | net.blobs_x(layer): \" % ( this_layer ,net.blobs_width(this_layer), net.blobs_height(this_layer) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feats_dict has now \"key\" values. However has not been initialized, it holds zeros\n",
    "feats_dict['conv1_1'][49999][63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 224, 224)\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# since we have no executed forward pass, is all zeros\n",
    "feat = net.blobs_data('conv1_1')\n",
    "print(feat.shape)\n",
    "i_feat = np.nonzero(feat)\n",
    "print(i_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0 layer:  conv1_1 feature map:  (10, 64, 224, 224)\n",
      "batch: 0 layer:  conv1_2 feature map:  (10, 64, 224, 224)\n",
      "batch: 0 layer:  conv2_1 feature map:  (10, 128, 112, 112)\n",
      "batch: 0 layer:  conv2_2 feature map:  (10, 128, 112, 112)\n",
      "batch: 0 layer:  conv3_1 feature map:  (10, 256, 56, 56)\n",
      "batch: 0 layer:  conv3_2 feature map:  (10, 256, 56, 56)\n",
      "batch: 0 layer:  conv3_3 feature map:  (10, 256, 56, 56)\n",
      "batch: 0 layer:  conv4_1 feature map:  (10, 512, 28, 28)\n",
      "batch: 0 layer:  conv4_2 feature map:  (10, 512, 28, 28)\n",
      "batch: 0 layer:  conv4_3 feature map:  (10, 512, 28, 28)\n",
      "batch: 0 layer:  conv5_1 feature map:  (10, 512, 14, 14)\n",
      "batch: 0 layer:  conv5_2 feature map:  (10, 512, 14, 14)\n",
      "batch: 0 layer:  conv5_3 feature map:  (10, 512, 14, 14)\n",
      "batch: 1 layer:  conv1_1 feature map:  (10, 64, 224, 224)\n",
      "batch: 1 layer:  conv1_2 feature map:  (10, 64, 224, 224)\n",
      "batch: 1 layer:  conv2_1 feature map:  (10, 128, 112, 112)\n",
      "batch: 1 layer:  conv2_2 feature map:  (10, 128, 112, 112)\n",
      "batch: 1 layer:  conv3_1 feature map:  (10, 256, 56, 56)\n",
      "batch: 1 layer:  conv3_2 feature map:  (10, 256, 56, 56)\n",
      "batch: 1 layer:  conv3_3 feature map:  (10, 256, 56, 56)\n",
      "batch: 1 layer:  conv4_1 feature map:  (10, 512, 28, 28)\n",
      "batch: 1 layer:  conv4_2 feature map:  (10, 512, 28, 28)\n",
      "batch: 1 layer:  conv4_3 feature map:  (10, 512, 28, 28)\n",
      "batch: 1 layer:  conv5_1 feature map:  (10, 512, 14, 14)\n",
      "batch: 1 layer:  conv5_2 feature map:  (10, 512, 14, 14)\n",
      "batch: 1 layer:  conv5_3 feature map:  (10, 512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "DEBUG_Mario = 1\n",
    "\n",
    "runforn = dcfgs.nBatches_fc if dcfgs.dic.fitfc else dcfgs.nBatches\n",
    "runforn = 2 # Override runforn for testing\n",
    "for batch in range(runforn):\n",
    "    if save:\n",
    "        \"\"\" first copy the data layer content into the points_dict\"\"\"\n",
    "        if not frozen_points:\n",
    "            net.forward()\n",
    "            #print(\"net forward, frozen_point is false\")\n",
    "            set_points_dict((batch, 0), net.data().copy())\n",
    "            set_points_dict((batch, 1), net.label().copy())\n",
    "        else:\n",
    "            net.net.set_input_arrays(points_dict[(batch, 0)], points_dict[(batch, 1)])\n",
    "            #print(\"net forward, frozen_point is true\")\n",
    "            net.forward()\n",
    "    else:\n",
    "        net.forward()\n",
    "        #print(\"net forward, no save\")\n",
    "     \n",
    "    \"\"\" then get the  activation of each layer \"\"\"\n",
    "    for name in names:\n",
    "        # pad = pads[name]\n",
    "        shape = shapes[name]\n",
    "        feat = net.blobs_data(name)\n",
    "        print(\"batch:\", batch, \"layer: \", name, \"feature map: \", net.blobs_shape(name))\n",
    "        \n",
    "        if inner or len(net.blobs_shape(name))==2 or (shape[0] == 1 and shape[1] == 1):\n",
    "            print(\"deal with FC layer(?) name: \", name)\n",
    "            feats_dict[name][fc_idx:(fc_idx + nPicsPerBatch)] = feat.reshape((net.num, -1))\n",
    "            continue\n",
    "        if batch >= dcfgs.nBatches and name in net.convs:\n",
    "            print(\"prevent the code bellow from executing if we exceed nBatchs(?)\" )\n",
    "            continue \n",
    "        # TODO!!! different patch for different image per batch\n",
    "        if save:\n",
    "            if not frozen_points or (batch, name, \"randx\") not in points_dict:\n",
    "                #embed()\n",
    "                \"\"\" and create random coordinates for the sampling process \"\"\"\n",
    "                randx = np.random.randint(0, shape[0]-0, nPointsPerLayer)\n",
    "                randy = np.random.randint(0, shape[1]-0, nPointsPerLayer)\n",
    "              \n",
    "                if dcfgs.dic.option == cfgs.pruning_options.resnet:\n",
    "                    if DEBUG_Mario: print(\"this line executed when dcfgs.dic.option is resnet\")\n",
    "                    branchrandxy = None\n",
    "                    branch1name = '_branch1'\n",
    "                    branch2cname = '_branch2c'\n",
    "                    if name in self.sums:\n",
    "                        #embed()\n",
    "                        nextblock = self.sums[self.sums.index(name)+1]\n",
    "                        nextb1 = nextblock + branch1name\n",
    "                        if not nextb1 in names:\n",
    "                            # the previous sum and branch2c will be identical\n",
    "                            branchrandxy = nextblock + branch2cname\n",
    "                    elif name in self.bns:\n",
    "                        if dcfgs.model == cfgs.Models.xception:\n",
    "                            branchrandxy = 'interstellar' + name.split('bn')[1].split('_')[0] + branch2cname\n",
    "                        elif dcfgs.model == cfgs.Models.resnet:\n",
    "                            branchrandxy = 'res' + name.split('bn')[1].split('_')[0] + branch2cname\n",
    "                            #print(\"correpondance\", branchrandxy)\n",
    "                    if branchrandxy is not None:\n",
    "                        if 0: print('pointsdict of', branchrandxy, 'identical with', name)\n",
    "                        randx = points_dict[(batch, branchrandxy , \"randx\")]\n",
    "                        randy = points_dict[(batch, branchrandxy , \"randy\")]\n",
    "               \n",
    "                if name.endswith('_conv1') and dcfgs.dic.option == 1:\n",
    "                    if DEBUG_Mario: print(\"this line executed because dcfgs.dic.option is 1\")\n",
    "                    fsums = ['first_conv'] + self.sums\n",
    "                    blockname = name.partition('_conv1')[0]\n",
    "                    nextb1  = fsums[fsums.index(blockname+'_sum')-1]\n",
    "                    branch1name = blockname + '_proj'\n",
    "                    if branch1name in self.convs:\n",
    "                        nextb1 = branch1name\n",
    "                    randx = points_dict[(batch, nextb1 , \"randx\")]\n",
    "                    randy = points_dict[(batch, nextb1 , \"randy\")]\n",
    "                    \n",
    "                # add the randome coordinates to the points_dict\n",
    "                set_points_dict((batch, name, \"randx\"), randx.copy())\n",
    "                set_points_dict((batch, name, \"randy\"), randy.copy())\n",
    "\n",
    "            else:\n",
    "                randx = points_dict[(batch, name, \"randx\")]\n",
    "                randy = points_dict[(batch, name, \"randy\")]\n",
    "        else:\n",
    "            randx = np.random.randint(0, shape[0]-0, nPointsPerLayer)\n",
    "            randy = np.random.randint(0, shape[1]-0, nPointsPerLayer)\n",
    "\n",
    "        for point, x, y in zip(range(nPointsPerLayer), randx, randy):\n",
    "            \"\"\" Finally, use the random coordinates to sample points from each channel in a layer\n",
    "            (10 points per channel, per each image per  batch). If we have :\n",
    "                |64 channels in a layer\n",
    "                |10 images per batch (validation dataset)\n",
    "                |500 batches (default in the code )\n",
    "                |\n",
    "                +-----> Then we will sample 5000 images, and a 10 point per each channel,\n",
    "                        (50000, 64) shaped array of sampled points \n",
    "            \"\"\"\n",
    "            i_from = idx+point*nPicsPerBatch\n",
    "            try:\n",
    "                feats_dict[name][i_from:(i_from + nPicsPerBatch)] = feat[:,:,x, y].reshape((net.num, -1))\n",
    "            except:\n",
    "                 print('total', runforn, 'batch', batch, 'from', i_from, 'to', i_from + nPicsPerBatch)\n",
    "                 raise Exception(\"out of bound\")\n",
    "        if DEBUG:\n",
    "            embed()\n",
    "    \n",
    "    idx += nFeatsPerBatch\n",
    "    fc_idx += nPicsPerBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat.shape:  (10, 64, 224, 224)\n",
      "no zero elements of feat:  (array([0, 0, 0, ..., 9, 9, 9]), array([ 0,  0,  0, ..., 63, 63, 63]), array([  0,   0,   0, ..., 223, 223, 223]), array([  0,   1,   2, ..., 221, 222, 223]))\n",
      "--------- Feature Map Values ----------\n",
      "channel shape:  (224, 224)\n",
      "--------- Feature Samples Dictionary ----------\n",
      "feats_dict['conv1_1'].shape:  (50000, 64)\n",
      "feats_dict['conv1_1'][0][63] was modified:  2.02482795715\n",
      "feats_dict['conv1_1'][49999][63] was not:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Now the activation (feaure maps) has been initialzied\n",
    "feat = net.blobs_data('conv1_1')\n",
    "print(\"feat.shape: \", feat.shape)\n",
    "i_feat = np.nonzero(feat)\n",
    "print(\"no zero elements of feat: \", i_feat)\n",
    "\n",
    "print(\"--------- Feature Map Values ----------\" )\n",
    "channel = feat[9][63]\n",
    "#channel = feat[10][63] is out of bound because there is only 9 images per batch  (0 indexing)\n",
    "#channel = feat[9][64] is out of bound because there is only 64 channels in conv1_1 (0 indexing)\n",
    "print(\"channel shape: \", channel.shape)\n",
    "#print(channel)\n",
    "\n",
    "if runforn == 2:\n",
    "    # feats_dict has been updated with new random samples \n",
    "    # however, we only ran for 2 batches so a lot of entries are still zero\n",
    "    print(\"--------- Feature Samples Dictionary ----------\" )\n",
    "    print(\"feats_dict['conv1_1'].shape: \", feats_dict['conv1_1'].shape)\n",
    "    print(\"feats_dict['conv1_1'][0][63] was modified: \", feats_dict['conv1_1'][0][63])\n",
    "    print(\"feats_dict['conv1_1'][49999][63] was not: \",feats_dict['conv1_1'][49999][63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats_dict: {'conv4_1': array([[ -404.42437744,   282.1819458 ,  -179.07402039, ...,\n",
      "        -2077.56469727,  1370.03405762, -1473.65600586],\n",
      "       [    2.15681624,   -23.00135612,  -101.84294128, ...,\n",
      "           57.3445816 ,    64.9425354 ,   199.47135925],\n",
      "       [-1107.15710449,  -572.66339111,  1992.01806641, ...,\n",
      "         -520.57092285,   805.49700928,   231.48554993],\n",
      "       ..., \n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ]]), 'conv1_2': array([[ 101.15026093,   58.07224655,  -63.68658447, ...,   50.85097122,\n",
      "         -33.3972702 ,  -17.4185009 ],\n",
      "       [  84.10009003,  293.05969238,  639.12677002, ...,   76.52697754,\n",
      "        -229.31529236,   57.52775574],\n",
      "       [ -76.34429932,  182.95185852,  243.95957947, ...,  162.76293945,\n",
      "          66.90992737,  -39.14985275],\n",
      "       ..., \n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ]]), 'conv5_2': array([[-167.37393188,  -11.85471344,   80.65695953, ..., -124.0870285 ,\n",
      "        -165.0138855 , -141.10089111],\n",
      "       [-121.47897339,  -82.68370056, -126.77896881, ...,  -67.84934998,\n",
      "         -63.59072876, -132.98580933],\n",
      "       [  33.07526398, -128.49420166, -151.66062927, ...,   -6.28757954,\n",
      "         -25.02073288,  101.96017456],\n",
      "       ..., \n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ]]), 'conv5_3': array([[ -93.92605591,  -88.78253174,  -54.01681519, ...,  -62.25275421,\n",
      "         -61.07686996,  -45.84748459],\n",
      "       [ -80.41545105, -136.56689453,  -44.66840363, ...,  -65.16105652,\n",
      "         -69.62593842,  -92.78823853],\n",
      "       [ -38.98820877,  -41.33043671,  -30.53673744, ...,  -50.34632111,\n",
      "         -74.83320618,  -12.03778553],\n",
      "       ..., \n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ]]), 'conv3_1': array([[-2217.48242188,   365.44503784, -1510.62243652, ...,\n",
      "         1104.77392578,  1254.6282959 ,   599.93481445],\n",
      "       [  664.97583008, -2834.95239258,  -417.32409668, ...,\n",
      "          -12.64254761,  1305.73657227,   275.69168091],\n",
      "       [  179.73748779, -3118.95776367,  -672.35998535, ...,\n",
      "         -199.25939941, -1424.30749512,   407.28512573],\n",
      "       ..., \n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ]]), 'conv3_2': array([[ 1576.47265625,   308.77819824,   242.36682129, ...,\n",
      "          319.00448608,   533.77575684,    16.31284523],\n",
      "       [  549.40979004,  -904.1282959 ,   -98.09667969, ...,\n",
      "         -396.3449707 ,  1694.35388184,  -357.64022827],\n",
      "       [  561.95495605,   242.88925171,   285.83270264, ...,\n",
      "         -276.40454102,  -469.87823486, -2265.43505859],\n",
      "       ..., \n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ]]), 'conv2_2': array([[ -389.71264648,  -442.99203491,   -49.51759338, ...,\n",
      "          448.67712402,    37.17575836,    44.502491  ],\n",
      "       [  594.29180908, -1080.1204834 ,   686.66296387, ...,\n",
      "         1166.78051758,   602.29888916, -1846.8092041 ],\n",
      "       [ -377.42190552,  -134.99720764,   -74.34122467, ...,\n",
      "          337.72375488, -1482.03820801,  2486.14672852],\n",
      "       ..., \n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ]]), 'conv4_2': array([[-1245.99121094,   176.15206909,  -907.05737305, ...,\n",
      "         -508.53683472,  -623.59759521, -1595.57348633],\n",
      "       [ -801.00598145,   280.96566772,  -868.81762695, ...,\n",
      "          -35.52109528,  -523.06164551,  -464.35534668],\n",
      "       [-1028.36010742,  -456.61410522,   303.09103394, ...,\n",
      "        -1129.39025879, -1458.41259766, -1734.12976074],\n",
      "       ..., \n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ]]), 'conv4_3': array([[-387.42623901, -522.58190918, -239.76568604, ..., -606.72705078,\n",
      "        -439.49783325, -111.48081207],\n",
      "       [-424.05725098, -388.50015259, -710.05297852, ...,  239.5256958 ,\n",
      "        -545.15222168, -522.9130249 ],\n",
      "       [-346.33773804, -165.70202637, -106.16744995, ...,  520.52288818,\n",
      "        -473.91213989,   -3.60728717],\n",
      "       ..., \n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ]]), 'conv2_1': array([[ -387.21865845,     8.67066574,   259.34451294, ...,\n",
      "          223.0401001 ,   -89.78185272,    20.7047081 ],\n",
      "       [ -768.49353027,  -245.45671082,   114.14134979, ...,\n",
      "         1735.60546875,  -305.85290527,     9.9770689 ],\n",
      "       [  125.25452423,    16.91031647,  -506.10824585, ...,\n",
      "           21.56059265,   -50.74615097,   161.25021362],\n",
      "       ..., \n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ]]), 'conv1_1': array([[  -1.1371057 ,   11.32094669,   21.18766403, ..., -125.17889404,\n",
      "          -9.44692326,    2.02482796],\n",
      "       [  16.47793388,  127.48031616,   -1.59211779, ...,  142.3870697 ,\n",
      "           6.61149883,   41.03870392],\n",
      "       [ -45.97873306,    0.21947467,  -31.73637009, ...,  123.18711853,\n",
      "          17.11903572,  -10.76282215],\n",
      "       ..., \n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ]]), 'conv5_1': array([[ -55.29928207, -140.75010681,  -48.48047256, ...,  -66.53433228,\n",
      "        -137.67556763, -285.60232544],\n",
      "       [-313.1696167 ,   85.88332367, -238.16090393, ..., -326.53384399,\n",
      "        -244.47389221, -177.82972717],\n",
      "       [-250.08821106,   80.8186264 ,  -67.12071228, ...,  215.37756348,\n",
      "        -335.03921509, -359.24957275],\n",
      "       ..., \n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    0.        ,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ]]), 'conv3_3': array([[  936.42425537, -1019.54205322, -1014.32348633, ...,\n",
      "         -172.08766174,  -823.15161133,  1050.61560059],\n",
      "       [ -332.56082153,   171.02082825,  -575.82946777, ...,\n",
      "         -914.32788086,   133.77764893,  -436.91177368],\n",
      "       [-1388.08874512,   865.79779053,  -330.1257019 , ...,\n",
      "        -1667.90808105,  -419.62649536, -2723.54418945],\n",
      "       ..., \n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ],\n",
      "       [    0.        ,     0.        ,     0.        , ...,\n",
      "            0.        ,     0.        ,     0.        ]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"feats_dict:\", feats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data layer values for batch 1 (shape): (10, 3, 224, 224)\n",
      "data layer labels for batch 1 (shape): (10,)\n",
      "batch 1, conv1_1, randx:  [131  95  19 223  70 158 167 103   0 120]\n",
      "batch 1, conv1_1, randy:  [ 67  50   9   0 215  12  59 124 170  41]\n"
     ]
    }
   ],
   "source": [
    "# points_dict was also modified. We added the random corrdinates to sample each batch\n",
    "# we alse added the data layer values a\n",
    "print(\"data layer values for batch 1 (shape):\", points_dict[(0,0)].shape)\n",
    "print(\"data layer labels for batch 1 (shape):\", points_dict[(0,1)].shape)\n",
    "\n",
    " \n",
    "print(\"batch 1, conv1_1, randx: \",points_dict[(1,'conv1_1','randx')])\n",
    "print(\"batch 1, conv1_1, randy: \",points_dict[(1,'conv1_1','randy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for conv1_1, feat.shape is : (10, 64, 224, 224)\n",
      "sample feature's array shape:  (10, 64)\n",
      "--------\n",
      "index : 0\n",
      "sampled features (channel 1): [ 14.36517906   5.88363218 -10.3242197  -10.01940441   0.73159975\n",
      "  -4.55395794   6.04811811  -1.29261112  10.54183674  -9.22943974]\n",
      "index : 10\n",
      "sampled features (channel 1): [-26.24725342   9.58125591 -10.24522495 -12.58506775 -66.23661804\n",
      "  -8.05879498   3.82703972  -1.3037535   10.35839558 -10.05900192]\n",
      "index : 20\n",
      "sampled features (channel 1): [-28.89755058  10.39723682 -10.46149349 -10.00550842  43.90003586\n",
      "  -5.3795166    0.75988054  -8.75227547  10.44703293 -10.49456596]\n",
      "index : 30\n",
      "sampled features (channel 1): [-14.58647442   9.30111122 -10.41462612 -10.37274647 -17.75824165\n",
      "  -8.36483383   1.99208069 -48.1091423   10.49388504  -9.47496319]\n",
      "index : 40\n",
      "sampled features (channel 1): [ 12.2435379    9.41791153 -10.42825127 -10.96895981  20.88611031\n",
      "  -5.66060257   1.93174982  12.44420719  10.21056461  -9.79193211]\n",
      "index : 50\n",
      "sampled features (channel 1): [   6.2536397   139.23298645 -144.45149231 -139.29449463 -101.31828308\n",
      "   40.21255112  117.36766815  -27.58049774  156.82575989 -143.22439575]\n",
      "index : 60\n",
      "sampled features (channel 1): [ -23.95565605  136.31686401 -143.62878418 -139.02937317   50.27421951\n",
      "   36.57699585  121.62010193 -103.54615784  156.77072144 -141.7183075 ]\n",
      "index : 70\n",
      "sampled features (channel 1): [ -6.97126675   7.87143564 -10.46885204 -10.16404819  32.52293396\n",
      "  -4.0684309    3.59907103 -12.81323433   9.61314011 -10.37374687]\n",
      "index : 80\n",
      "sampled features (channel 1): [ -9.77684879   9.5582943  -10.01728725  -9.3973999   21.0151825\n",
      "  -6.80518389   3.90197515  59.57605743  10.14847469 -11.07013512]\n",
      "index : 90\n",
      "sampled features (channel 1): [ -9.65317822  11.5266037  -10.16845703 -12.0603056   67.41182709\n",
      "  -5.55315208   2.33362174  -4.99214172  10.60713005  -9.982234  ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" examine feat \"\"\"\n",
    "#my_dict = feats_dict\n",
    "feat = net.blobs_data('conv1_1')\n",
    "\n",
    "print(\"for conv1_1, feat.shape is :\",feat.shape) \n",
    "for point, x, y in zip(range(nPointsPerLayer), randx, randy):\n",
    "# use the random coordinates to sample from feats (activation blobs?) into the feats_dict    \n",
    "    i_from = idx+point*nPicsPerBatch\n",
    "    \"\"\"NOTE:\n",
    "            For each  image , we extract 10 points points from each layer channel\"\"\"\n",
    "    a = feat[:,:,x, y].reshape((net.num, -1)) \n",
    "     \n",
    "    if i_from == 0:\n",
    "        print(\"sample feature's array shape: \", a.shape)\n",
    "        print(\"--------\")\n",
    "    print(\"index :\", i_from)  \n",
    "    print(\"sampled features (channel 1):\", a[:,63]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 100.000\n",
      "[fronze_points is False] return feats_dict, points_dict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'end of net.extract_features'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcfgs.data = cfgs.Data.lmdb # this simple resets the config (why?)\n",
    "net.clr_acc()\n",
    "if save:\n",
    "    if frozen_points:\n",
    "        if points_dict is not None:\n",
    "            print(\"return feats_dict, points_dict\")\n",
    "            #return feats_dict, points_dict\n",
    "        #return feats_dict\n",
    "        print(\"[points_dict is None] return feats_dict\")\n",
    "    else:\n",
    "        print(\"[fronze_points is False] return feats_dict, points_dict\")\n",
    "        #return feats_dict, points_dict\n",
    "else:\n",
    "    print(\"[save is false] return feats_dict\")\n",
    "    #return feats_dict\n",
    "\"\"\"end of net.extract_features\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of function call: feats_dict, points_dict = net.extract_features(names=convs, save=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something happened here\n",
      "wrote memory data layer to temp/mem_bn_vgg.prototxt\n",
      "freezing imgs to temp/frozen500.pickle\n",
      "temp/frozen500.pickle\n"
     ]
    }
   ],
   "source": [
    "data_layer = net.data_layer\n",
    "if len(net.net_param_layer(data_layer)) == 2: # this call reaaches the NetBuilder for\n",
    "    if DEBUG_Mario: print(\"something happened here\")\n",
    "    net.net_param.net.layer.remove(net.get_layer(data_layer))\n",
    "\n",
    "# we will prepare the data laye to run with MemoryData type\n",
    "i = net.get_layer(data_layer)\n",
    "i.type = \"MemoryData\"\n",
    "i.memory_data_param.batch_size = points_dict['data'][0]\n",
    "i.memory_data_param.channels = points_dict['data'][1]\n",
    "i.memory_data_param.height = points_dict['data'][2]\n",
    "i.memory_data_param.width = points_dict['data'][3]\n",
    "i.ClearField(\"transform_param\")\n",
    "i.ClearField(\"data_param\")\n",
    "i.ClearField(\"include\")\n",
    "\n",
    "print(\"wrote memory data layer to\", net.save_pt(prefix=\"mem\"))\n",
    "print(\"freezing imgs to\", frozen)\n",
    "\n",
    "if cfgs.layer:\n",
    "    if DEBUG_Mario: print(\"operation on single-layer pruning\") \n",
    "    \n",
    "    def subfile(filename):\n",
    "        return osp.join(frozen, filename)\n",
    "    \n",
    "    with open(subfile(net._points_dict_name), 'wb') as f:\n",
    "        print(\"dumping points_dict\")\n",
    "        pickle.dump(points_dict, f, protocol=net._protocol)\n",
    "    if DEBUG_Mario: print(\"Will dump sampled features (feats_dict) of each layer independantly\") \n",
    "    for conv in convs:\n",
    "        with open(subfile(conv), 'wb') as f:\n",
    "            print(\"dumping \"+conv)\n",
    "            pickle.dump(feats_dict[conv], f, protocol=net._protocol)\n",
    "\n",
    "else:\n",
    "    with open(frozen, 'wb') as f:\n",
    "        pickle.dump([feats_dict, points_dict], f, protocol=net._protocol)\n",
    "\n",
    "#return frozen\n",
    "print(frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of function call:  net.freeze_images(check_exist=check_exist, convs=convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return {'model': model}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'end of step1 execution'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"return {'model': model}\")\n",
    "outputs = {\"model\":model}\n",
    "\"\"\"end of step1 execution\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "stage3 speed 3.0\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "perform pruning: solve()\n"
     ]
    }
   ],
   "source": [
    "if DEBUG_Mario: print(\"output returned from step 1: \", outputs) \n",
    "printstage(\"speed \", dcfgs.dic.keep)\n",
    "outputs['pt'] = mem_pt(pt) # update pt to point to the new prototxt (with data layer of  type MemoryData)\n",
    "if DEBUG_Mario: print(\"output modified\" ,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuction call:  outputs = solve(**outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "solve execution\n",
    "def solve(pt, model)\n",
    "\"\"\"\n",
    "\n",
    "# what is the value of pt? and model?\n",
    "# check the excution of worker.do() to find out how it extracts parameters from **outputs\n",
    "pt = outputs['pt'] #?\n",
    "model = outputs['model'] #?\n",
    "\n",
    "net = Net(pt, model=model)\n",
    "net.load_frozen() # this method can load images from memory if we pass a feats_dic. For what?\n",
    "\n",
    "WPQ, new_pt = net.R3()\n",
    "print(\"return {'WPQ': WPQ, 'new_pt': new_pt}\")\n",
    "\"\"\"end of solve execution\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of function call: outputs = solve(**outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {\"WPQ\": WPQ, \"new_pt\": new_pt}\n",
    "if DEBUG_Mario: print(\"output returned from solve : \", outputs) \n",
    "printstage(\"saving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuction call:  outputs = stepend(model=model, **outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stepned execution\n",
    "def stepend(new_pt, model, WPQ):\n",
    "\"\"\"\n",
    "# what is the value of new_pt? and WPQ?\n",
    "# check the excution of worker.do() to find out how it extracts parameters from **outputs\n",
    "new_pt = outputs['new_pt'] #?\n",
    "WPQ = outputs['WPQ'] #?\n",
    "model = cfgs.vgg.weights #?\n",
    "\n",
    "net = Net(new_pt, model=model)\n",
    "net.WPQ = WPQ\n",
    "net.finalmodel(save=False) # load weights into the caffemodel -by Mario\n",
    "net.dis_memory()\n",
    "\n",
    "#final = net.finalmodel(WPQ, prefix='3r')\n",
    "new_pt, new_model = net.save(prefix='3c')\n",
    "\n",
    "print('caffe test -model',new_pt, '-weights',new_model)\n",
    "print(\"return {'final': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of fuction call:  outputs = stepend(model=model, **outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {'final': None}\n",
    "if DEBUG_Mario: print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of function call: c3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\"experiment\")\n",
    "    parser.add_argument('-tf', dest='tf_vis', help='tf devices', default=None, type=str)\n",
    "    parser.add_argument('-caffe', dest='caffe_vis', help='caffe devices', default=None, type=str)\n",
    "    parser.add_argument('-action', dest='action', help='action', default='train', type=str)\n",
    "    attrs = ['dic', 'an', 'res']\n",
    "    for d in attrs:\n",
    "        for i in dcfgs[d]:\n",
    "            parser.add_argument('-'+d+'.'+i, dest=d+'DOT'+i, help=d+'.'+i, default=None,type=str)\n",
    "\n",
    "    for i in dcfgs:\n",
    "        if i not in attrs:\n",
    "            parser.add_argument('-'+i, dest=i, help=i, default=None,type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if args.tf_vis is not None: cfgs.tf_vis = args.tf_vis\n",
    "    if args.caffe_vis is not None: cfgs.caffe_vis = args.caffe_vis\n",
    "    for d in attrs:\n",
    "        for i in dcfgs[d]:\n",
    "            att = getattr(args, d+'DOT'+i)\n",
    "            if att is not None:\n",
    "                if 0:\n",
    "                    print(d,i, att)\n",
    "                dcfgs[d][i]=type(dcfgs[d][i])(att)\n",
    "\n",
    "    for i in dcfgs:\n",
    "        if i in attrs:\n",
    "            continue\n",
    "        att = getattr(args, i)\n",
    "        if att is not None:\n",
    "            dcfgs[i]=type(dcfgs[i])(att)\n",
    "\n",
    "    dcfgs.Action = args.action\n",
    "    if args.model is not None:\n",
    "        netmodel = getattr(cfgs, args.model)\n",
    "        cfgs.accname = netmodel.accname\n",
    "        if args.prototxt is None:\n",
    "            dcfgs.prototxt = netmodel.model\n",
    "        if args.weights is None:\n",
    "            dcfgs.weights = netmodel.weights\n",
    "    return args\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default code (except arg parser) execution test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "stage0 freeze\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "temp/bn_vgg.prototxt\n",
      "\u001b[31mincluding last conv layer!\u001b[0m\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv1_1 (5000, 64)\n",
      "Extracting conv1_2 (5000, 64)\n",
      "Extracting conv2_1 (5000, 128)\n",
      "Extracting conv2_2 (5000, 128)\n",
      "Extracting conv3_1 (5000, 256)\n",
      "Extracting conv3_2 (5000, 256)\n",
      "Extracting conv3_3 (5000, 256)\n",
      "Extracting conv4_1 (5000, 512)\n",
      "Extracting conv4_2 (5000, 512)\n",
      "Extracting conv4_3 (5000, 512)\n",
      "Extracting conv5_1 (5000, 512)\n",
      "Extracting conv5_2 (5000, 512)\n",
      "Extracting conv5_3 (5000, 512)\n",
      "Acc  90.200\n",
      "wrote memory data layer to temp/mem_bn_vgg.prototxt\n",
      "freezing imgs to temp/frozen500.pickle\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "stage1 speed3.0\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "loading imgs from temp/frozen500.pickle\n",
      "loaded\n",
      "Extracting X relu1_1 From Y conv1_2 stride 1\n",
      "Acc  90.200\n",
      "spatial_decomposition 9.814770460128784\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv1_2 (5000, 64)\n",
      "Acc  89.800\n",
      "Reconstruction Err 0.0135935890418\n",
      "channel_decomposition 7.794588088989258\n",
      "Extracting X pool1 From Y conv2_1 stride 1\n",
      "Acc  89.800\n",
      "DEBUG net.dictionary_kernel: dcfgs.ls is not gd or there is no memory data -by Mario\n",
      "rMSE 0.0150757869366\n",
      "channel_pruning 8.733855724334717\n",
      "Extracting X pool1 From Y conv2_1 stride 1\n",
      "Acc  89.800\n",
      "spatial_decomposition 8.785545825958252\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv2_1 (5000, 128)\n",
      "Acc  89.800\n",
      "Reconstruction Err 0.0209236324653\n",
      "channel_decomposition 8.882300853729248\n",
      "Extracting X conv2_1 From Y conv2_2 stride 1\n",
      "Acc  89.800\n",
      "DEBUG net.dictionary_kernel: dcfgs.ls is not gd or there is no memory data -by Mario\n",
      "rMSE 0.0231891194269\n",
      "channel_pruning 8.939832210540771\n",
      "Extracting X relu2_1 From Y conv2_2 stride 1\n",
      "Acc  90.000\n",
      "spatial_decomposition 10.808913230895996\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv2_2 (5000, 128)\n",
      "Acc  90.400\n",
      "Reconstruction Err 0.0542451203057\n",
      "channel_decomposition 9.554548263549805\n",
      "Extracting X pool2 From Y conv3_1 stride 1\n",
      "Acc  90.200\n",
      "DEBUG net.dictionary_kernel: dcfgs.ls is not gd or there is no memory data -by Mario\n",
      "rMSE 0.0471926949245\n",
      "channel_pruning 10.91722297668457\n",
      "Extracting X pool2 From Y conv3_1 stride 1\n",
      "Acc  90.000\n",
      "spatial_decomposition 14.457913637161255\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv3_1 (5000, 256)\n",
      "Acc  89.800\n",
      "Reconstruction Err 0.0591745094008\n",
      "channel_decomposition 18.018824577331543\n",
      "Extracting X conv3_1 From Y conv3_2 stride 1\n",
      "Acc  90.200\n",
      "DEBUG net.dictionary_kernel: dcfgs.ls is not gd or there is no memory data -by Mario\n",
      "rMSE 0.0578525114164\n",
      "channel_pruning 11.169920444488525\n",
      "Extracting X relu3_1 From Y conv3_2 stride 1\n",
      "Acc  90.000\n",
      "spatial_decomposition 17.283509016036987\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv3_2 (5000, 256)\n",
      "Acc  89.600\n",
      "Reconstruction Err 0.113698582066\n",
      "channel_decomposition 18.77482795715332\n",
      "Extracting X conv3_2 From Y conv3_3 stride 1\n",
      "Acc  89.600\n",
      "DEBUG net.dictionary_kernel: dcfgs.ls is not gd or there is no memory data -by Mario\n",
      "rMSE 0.104931312206\n",
      "channel_pruning 13.359165906906128\n",
      "Extracting X relu3_2 From Y conv3_3 stride 1\n",
      "Acc  89.400\n",
      "spatial_decomposition 19.099756002426147\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv3_3 (5000, 256)\n",
      "Acc  89.600\n",
      "Reconstruction Err 0.132116489239\n",
      "\u001b[31m1e2 exceed\u001b[0m\n",
      "channel_decomposition 19.008370399475098\n",
      "Extracting X pool3 From Y conv4_1 stride 1\n",
      "Acc  89.200\n",
      "spatial_decomposition 36.74852776527405\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv4_1 (5000, 512)\n",
      "Acc  89.000\n",
      "Reconstruction Err 0.107314763061\n",
      "\u001b[31m1e2 exceed\u001b[0m\n",
      "channel_decomposition 67.55459785461426\n",
      "Extracting X conv4_1 From Y conv4_2 stride 1\n",
      "Acc  89.000\n",
      "DEBUG net.dictionary_kernel: dcfgs.ls is not gd or there is no memory data -by Mario\n",
      "rMSE 0.12518857532\n",
      "channel_pruning 44.041499614715576\n",
      "Extracting X relu4_1 From Y conv4_2 stride 1\n",
      "Acc  88.200\n",
      "spatial_decomposition 58.105509519577026\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv4_2 (5000, 512)\n",
      "Acc  87.600\n",
      "Reconstruction Err 0.218065607783\n",
      "\u001b[31m1e2 exceed\u001b[0m\n",
      "channel_decomposition 72.05327558517456\n",
      "Extracting X conv4_2 From Y conv4_3 stride 1\n",
      "Acc  87.600\n",
      "DEBUG net.dictionary_kernel: dcfgs.ls is not gd or there is no memory data -by Mario\n",
      "rMSE 0.286248400909\n",
      "channel_pruning 38.2668776512146\n",
      "Extracting X relu4_2 From Y conv4_3 stride 1\n",
      "Acc  74.800\n",
      "spatial_decomposition 61.997945070266724\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv4_3 (5000, 512)\n",
      "Acc  69.400\n",
      "Reconstruction Err 0.178643136006\n",
      "\u001b[31m1e2 exceed\u001b[0m\n",
      "channel_decomposition 74.32484865188599\n",
      "Extracting X pool4 From Y conv5_1 stride 1\n",
      "Acc  71.800\n",
      "spatial_decomposition 93.2506844997406\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv5_1 (5000, 512)\n",
      "Acc  80.800\n",
      "Reconstruction Err 0.242450395002\n",
      "\u001b[31m1e2 exceed\u001b[0m\n",
      "channel_decomposition 88.6280312538147\n",
      "Extracting X relu5_1 From Y conv5_2 stride 1\n",
      "Acc  81.000\n",
      "spatial_decomposition 95.4514319896698\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv5_2 (5000, 512)\n",
      "Acc  80.200\n",
      "Reconstruction Err 0.235326551916\n",
      "channel_decomposition 86.86750268936157\n",
      "Extracting X relu5_2 From Y conv5_3 stride 1\n",
      "Acc  81.000\n",
      "spatial_decomposition 84.17870473861694\n",
      "run for 500 batches nFeatsPerBatch 10\n",
      "Extracting conv5_3 (5000, 512)\n",
      "Acc  79.200\n",
      "Reconstruction Err 0.107889528357\n",
      "channel_decomposition 83.60173416137695\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "stage2 saving\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "caffe test -model temp/3c_3C4x_mem_bn_vgg.prototxt -weights temp/3c_vgg.caffemodel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "    cfgs.set_nBatches(dcfgs.nBatches)\n",
    "\n",
    "    dcfgs.dic.option=1\n",
    "\n",
    "    if args.action == cfgs.Action.addbn:\n",
    "        addbn(pt=dcfgs.prototxt, model=dcfgs.weights)\n",
    "\n",
    "    elif args.action == cfgs.Action.splitrelu:\n",
    "        splitrelu()\n",
    "\n",
    "    elif args.action == cfgs.Action.c3:\n",
    "        c3()\n",
    "    elif args.action == cfgs.Action.combine:\n",
    "        combine()\n",
    "    else:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "cfgs.set_nBatches(dcfgs.nBatches)\n",
    "dcfgs.dic.option=1\n",
    "\n",
    "#desired action is c3(), so ...\n",
    "c3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
